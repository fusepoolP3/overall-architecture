# Integration of SILK component into platform

Deliverable 3.2

Delivery date: 15

## Document History

|Ver.          | Name         | Date         | Remark       |
|--------------|--------------|--------------|--------------|
|v0.1          | Luigi Selmi  | 2014-11-28   | First draft |


## Executive Summary

The 4th and last "expectation of behaviour" specified by Tim Berners-Lee in his proposal to create a web of data was
to "include links to other URIs so that they can discover more things". A particular class of these links are those that
connect different representations of the same thing. Being able to connect records coming from different data sources is
a fundamental objective of any data integration effort starting from putting together records of in different spreadsheets to integrating data assets owned by one or more companies and finally merging different views of a subject in the World Wide Web where the AAA slogan must apply: "Anyone can say Anything about Any topic".

The objective of the task has been to provide a support for disambiguating and interlinking entities that are added to a LDP container.

## Acronyms and Abbreviations



| Acronym |                 Description                  |
|---------|----------------------------------------------|
| API     | Application Programming Interface            |
| BUAS    | Bern University of Applied Sciences          |
| FP3     | Fusepool P3                                  |
| HTTP    | Hypertext Transfer Protocol                  |
| HTTPS   | Secure Hypertext Transfer Protocol           |
| IRI     | Internationalized Resource Identifier        |
| LDP     | Linked Data Platform                         |
| RDF     | Resource Description Framework               |
| RDFS    | RDF Schema                                   |
| REST    | Representational State Transfer              |
| URI     | Uniform Resource Identifier                  |
| URL     | Uniform Resource Locator                     |




## Normative namespaces

In this document the prefixes used in [CURIEs](http://www.w3.org/TR/curie/) shall refer the following
IRI prefixed:


| Prefix |                                                                                                                                                                                             Namespace                                                                                                                                                                                              |
|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| rdf    | [http://www.w3.org/1999/02/22-rdf-syntax-ns\#](http://www.w3.org/1999/02/22-rdf-syntax-ns)
| rdfs   | [http://www.w3.org/2000/01/rdf-schema\#](http://www.w3.org/2000/01/rdf-schema) |
| xsd    | [http://www.w3.org/2001/XMLSchema\#](http://www.w3.org/2001/XMLSchema)     |
| dct    | [http://purl.org/dc/terms/](http://purl.org/dc/terms/) |
| ldp    | [http://www.w3.org/ns/ldp\#](http://www.w3.org/ns/ldp#)|
| fp3    | [http://vocab.fusepool.info/fp3\#](http://vocab.fusepool.info/fp3#)|
| eldp   | [http://vocab.fusepool.info/eldp\#](http://vocab.fusepool.info/eldp#)   |
| trans  | [http://vocab.fusepool.info/transformer\#](http://vocab.fusepool.info/transformer#) |
| fam    | [http://vocab.fusepool.info/fam\#](http://vocab.fusepool.info/fam#) |

## Background
(Why was this task created, what is the goal of the subtask and how does it contribute to the deliverable )

This task has been proposed to offer a mechanism to a user of the platform to discover entities that have been given different names by different authors that refer to the same thing in order to merge their descriptions and augment the knowledge about the entity. This is a crucial task for any user of the platform since what she likely wants is not a collection of disconnected graphs but a well connected knowledge base. The task of disambiguating entities has been given different name by different communities: record linkage, deduplication, entity resolution and so forth. All the methodologies that have been developed to discover equivalent entities are based on the assumption of the identity of indiscernibles [2] that is attributed to the German philosopher Gottfried Wilhelm Leibniz, that states that there cannot be separate objects that have all their properties exactly the same. In order to ascertain whether two entities are the same two types of inferences can be evaluated. On one hand an identity can be inferred from functional and inverse functional properties when there are some statements that relates two entities and the properties are of the type mentioned. As an example, if the following facts are stated

:Bob :hasFather :John  
:Bob :hasFather :Johnny  

and we know that  

:hasFather rdf:type owl:FunctionalProperty  

the identity :John owl:sameAs :Johnny can be inferred by a reasoner. The problem is that it doesn't happen very often that an entity in a data set is related to an entity in a different data set through a functional property. Inverse functional properties like passport number, driver license fiscal codes, are used more frequently. As an example, if the following facts are stated

:John :hasPassport "A123456"  
:Johnny :hasPassport "A123456"

and we know that

:hasPassport rdf:type owl:InverseFunctionalProperty

then a reasoner can infer :John owl:sameAs :Johnny. That said, such properties are not always available and it can happen that the values contain errors so that it is always better to decide about the identity of two entities on more than one single property. In order to disambiguate two entities by their description, i.e. comparing their properties, we must analyze the following aspects

* the vocabulary used for the description  
* the set of properties that are considered to be enough to distinguish two entities globally  
* missing values, spelling errors and lexical differences in the properties literal values

Starting with the case in which all the entities are described using the same vocabulary, so that there is no ambiguity in the meaning of the properties, we have to decide which properties must be compared in order to distinguish two entities globally in the Semantic Web and Linked Data world. In the relational database world this decision is similar to the definition of a primary key for a tuple. The type of the entities is one of such property that is usually available or can be easily inferred. For entities like persons properties like name, place and date of birth are considered to be enough in most cases. For entities like organizations the same is for the name, the registered office or its address. In case some of these properties are missing we will have to consider the uncertainty in the value that is calculated as a result of the comparisons of all the properties used. Coming to the third point of the comparison issues, many algorithms have been proposed to determine the similarity between two strings. A similarity measure tells us in a numeric format which is the distance between two strings. Good overviews of such algorithms are given in [5] and [6]. In case the vocabulary used for the properties is different a mapping must be defined between the different terms used for the properties. The result of a comparison of two entities of the same type with semantically equivalent properties can be defined as the sum of all the similarity values between the properties used in the comparison of two entities. One property can be more relevant in the comparison of two entities so that it can be weighted. The result must be used to decide automatically if the two entities are a match or a non match or if a manual intervention is needed to disambiguate the two entities. A matching or linkage rule can be defined setting upper and lower threshold values so that a comparison result above the upper value will trigger a match, a result below the lower threshold will trigger a non- match and finally a value between the lower and upper thresholds will be a possible link that will require a human intervention to be solved. The first theoretical framework in the field of record linkage was proposed by Fellegi and Sunter in [7]. It can happen that a match has been determined while the two entities refer to different objects or that a non match has been found for entities that are the same. These errors are referred to as false positives and false negatives respectively. The objective of the framework described in [7] was to define an optimal linkage rule in which after having set acceptable error levels for the false positives and false negatives it must be possible to minimize the number of possible links that require costly human intervention.

The distance between two property values decreases as long as two strings are different. After a pair comparison the pairs are indexed. It is then possible to chose an index n for which a comparison with index i < n will result in a match and another index n' for which the comparison with index i > n' will result in a non match. The distance at n represents the upper threshold while at n' represents the lower threshold.

The matching function is the aggregate value of the similarity functions used in comparisons. Each similarity function should return a value in the interval [0,1] in order to be aggregated with all the others used in a comparison. When a similarity function returns a value that is not in that range it can be normalized. As an example the Levenshtein distance returns the minimum number of edits needed to transform a string into the other. A normalized version can be defined as a linear function of the similarity that returns a value 0 if the number of edits equals a threshold and up to 1 when no edits are needed.

After a set of properties have been selected for the comparison task, we have to chose the similarity function that we want to apply when comparing two property values, the aggregation type of the properties comparison results (average, maximum, weighted average), the upper and lower thresholds for matches and non matches. Settings these parameters corresponds in statistics to making an hypothesis that all the matches and non matches will be found with no false positive and no false negatives. We have to test our hypothesis creating  a reference links set linking manually a sub set of entities from the source data set with entities from the target data set. When we run the rules we may find out that some of the links in the reference links set have not been found (false negatives) while some false links have been created (false positives). These two values are used to define precision and recall of the matching task

  P = TP / (TP + FP)  
  R = TP / (TP + FN)  

These values must be compared with the possible outcomes taking into account a known distribution of results (normal, binomial, t-distribution) and the significance level (5%) in order to accept or reject the null hypothesis. In our case the values are used to modify the linkage rules settings in order to get a result within the significance level.

As we do not want to calculate the ratio a methodology is to create a reference set of entities checked manually for their matches and non matches and try different linkage rules in order to find the one (optimal) that returns all the matches and non matches in the reference set.

* Define Precision and Recall in terms of true positives, false positive and false negatives

* The owl:sameAs relation: simmetry, reflexivity, transitivity  


Meaning of the property owl:sameAs:  
if two entities A and B are believed to be the same then, for substitution axiom, every sentence that is true of A must be true of B. Translated in RDF this principle entails that for each triple in which A is subject or object a new triple can be inferred by a reasoner with B in place of A and viceversa. The issue with the given semantics of the owl:sameAs property is that there might be good reasons (see [4]) to not believe all the claims about the individual A made by an external data provider and at the same time we are not assured our claims are shared by the external data provider if those claims were not already in the data set. The decision in the first case could depend on the provenance of the data set.  

Alignment of the properties used to describe the entities.  

Linkage rules, similarity measures (distance metrics), thresholds  

UC 1) A user wants to discover duplicates in a RDF data set  
UC 2) A user wants to interlink entities, like persons, organizations or places, in a container to entities in a public data  set

## Execution/Implementation
(Describe the strategy to implement the task, describe how the risk known upfront were taken into account, describe the actual implementation, describe all trial and error, discussions, failures etc.)

* Implement  proprietary solution  
* Choose a tool that provides common algorithms and metrics, configurable and allows extensions  
* Any solution must fit in the platform architecture  
* Two tools have been tested LIMES and SILK: description of the tools, configurability, performances, license  
* Issue with blocking function. Set the block so that the number of entities within each block is uniform otherwise th eproblem is simply moved from the entire set to the block that contains most of the entities.  
* SILK selected as it comes with Apache 2.0 license.  
  Description of the SILK framework (Silk Single Machine), Silk Link Specification Language and Silk Workbench.
* Issues with geographical data: differnt formats, ShapeFile, CSV, JSON, XML
* Issues with geographical data: coordinates in different reference system (WGS84, UTM)
* Issues with geographical data: Incosistent use of vocabularies
* Issues with geographical data: geocoding of addresses
* Performances. Multi-threaded comparisons. Given the number of cores available in the machine the number of threads can be set to use all the cores. The comparisons (match task) to be evaluated for all the entities in a block partition can be split in different threads. A new thread can be started when a partition is ready)  


## Solution
(Describe the solution in detail, start with the benefit the solution provides, then go to the technical details.)

* Implementation of the transformer (based on the REST architecture)
* Description of the tool (SILK)
* Description of the Linkage Rules (discriminative properties, distance measure,thresholds)
* Description of the transformer

## Future work
(Briefly outline how this solution or the way to implement it could be improved by others.)

* Use of other types of topological relationships by developing an extension for Jena or Virtuoso
* Use of OpenStreetMap or LinkedGeoData for interlinking  
* Use of SILK learning capabilities to define linkage rules  


## References
[1] [Tim Berners-Lee - Linked Data](http://www.w3.org/DesignIssues/LinkedData.html)  
[2] [Identity of indiscernibles](http://plato.stanford.edu/entries/identity-indiscernible/)  
[3] [D. Allemang, J. Hendler - Semantic Web for the Working Ontologist]  
[4] [H. Alpin, I. Herman, P. Hayes - When owl:sameAs isn't the Same: An Analysis of Identity Links on the Semantic Web ]  
[5] [A. Elmagarmid et al. - Duplicate Record Detection: A Survey]  
[6] [W. Winkler - Matching and Record Linkage]  
[7] [I. Fellegi, A. Sunter - A Theory For Record Linkage]  
